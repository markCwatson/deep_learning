{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Digit Recognizer\n\nThis is for Kaggle's [Digit Recognizer competition](https://www.kaggle.com/competitions/digit-recognizer)\n\nThis approach is based off of Andrew Ng's Neural Networks and Deep Learning course in the [Deep Learning specialization on Coursera](https://www.coursera.org/specializations/deep-learning); however, it has been adapted for this particular problem.\n\nThis approach includes a implementation of a deep neural network utilizing numpy.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport copy","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.849613Z","iopub.execute_input":"2023-12-17T19:20:40.849972Z","iopub.status.idle":"2023-12-17T19:20:40.854772Z","shell.execute_reply.started":"2023-12-17T19:20:40.849924Z","shell.execute_reply":"2023-12-17T19:20:40.853737Z"},"trusted":true},"execution_count":278,"outputs":[]},{"cell_type":"code","source":"def data_prep(raw, num_classes=10):\n    \"\"\"\n    Argument:\n    raw -- The raw data rad from a csv file\n    num_classes -- The number of labels\n\n    Returns:\n    x -- Normalized\n    y_one_hot -- One-hot encoding of labels\n    \"\"\"\n    data = np.array(raw)\n    m, n = data.shape\n    print(\"data.shape = (m = {0}, n = {1})\".format(m, n))\n    \n    # Extract labels and feature set\n    y = data[:, 0]\n    X = data[:, 1:] / 255\n    \n    # Transpose X to match the expected shape (number_of_features, number_of_examples)\n    X = X.T\n    \n    # Convert labels to one-hot encoding\n    y_one_hot = np.zeros((m, num_classes))\n    y_one_hot[np.arange(m), y] = 1\n\n    # Transpose y_one_hot to match the expected shape (number_of_classes, number_of_examples)\n    y_one_hot = y_one_hot.T\n    \n    print(\"X.shape = {0}\".format(X.shape))\n    print(\"y_one_hot.shape = {0}\".format(y_one_hot.shape))\n    \n    return X, y_one_hot","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.882204Z","iopub.execute_input":"2023-12-17T19:20:40.882532Z","iopub.status.idle":"2023-12-17T19:20:40.888705Z","shell.execute_reply.started":"2023-12-17T19:20:40.882506Z","shell.execute_reply":"2023-12-17T19:20:40.887656Z"},"trusted":true},"execution_count":279,"outputs":[]},{"cell_type":"code","source":"def relu(Z):\n    \"\"\"\n    Implements the ReLU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    A = np.maximum(0, Z)\n    cache = Z  \n    assert (A.shape == Z.shape)\n    \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.906314Z","iopub.execute_input":"2023-12-17T19:20:40.907141Z","iopub.status.idle":"2023-12-17T19:20:40.911633Z","shell.execute_reply.started":"2023-12-17T19:20:40.907102Z","shell.execute_reply":"2023-12-17T19:20:40.910773Z"},"trusted":true},"execution_count":280,"outputs":[]},{"cell_type":"code","source":"def softmax(Z):\n    \"\"\"\n    For multi-class classification.\n\n    Arguments:\n    Z -- numpy array of any shape\n\n    Returns:\n    A -- Softmax output, same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    # subtract the maximum of Z from each Z before applying exp to stabilize the computation.\n    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n    A = np.exp(Z_shifted) / np.sum(np.exp(Z_shifted), axis=0, keepdims=True)\n    assert (A.shape == Z_shifted.shape)\n    cache = A\n    \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.913075Z","iopub.execute_input":"2023-12-17T19:20:40.913712Z","iopub.status.idle":"2023-12-17T19:20:40.923223Z","shell.execute_reply.started":"2023-12-17T19:20:40.913689Z","shell.execute_reply":"2023-12-17T19:20:40.922200Z"},"trusted":true},"execution_count":281,"outputs":[]},{"cell_type":"code","source":"def initialize_parameters(layer_dims):\n    \"\"\"\n    Initialze using the He initialization method.\n    \n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n\n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n        Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n        bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    parameters = {}\n    L = len(layer_dims)\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2. / layer_dims[l - 1])\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n    return parameters","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.925633Z","iopub.execute_input":"2023-12-17T19:20:40.926083Z","iopub.status.idle":"2023-12-17T19:20:40.936805Z","shell.execute_reply.started":"2023-12-17T19:20:40.926059Z","shell.execute_reply":"2023-12-17T19:20:40.936243Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"code","source":"def linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W, A) + b\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.938093Z","iopub.execute_input":"2023-12-17T19:20:40.938427Z","iopub.status.idle":"2023-12-17T19:20:40.947019Z","shell.execute_reply.started":"2023-12-17T19:20:40.938407Z","shell.execute_reply":"2023-12-17T19:20:40.946309Z"},"trusted":true},"execution_count":283,"outputs":[]},{"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation=''):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"softmax\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n                stored for computing the backward pass efficiently\n    \"\"\"\n    Z, linear_cache = linear_forward(A_prev, W, b)\n        \n    match activation:\n        case \"softmax\":\n            A, activation_cache = softmax(Z)\n        case \"relu\":\n            A, activation_cache = relu(Z)\n        case _:\n            assert (False)\n        \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.947727Z","iopub.execute_input":"2023-12-17T19:20:40.948634Z","iopub.status.idle":"2023-12-17T19:20:40.962104Z","shell.execute_reply.started":"2023-12-17T19:20:40.948610Z","shell.execute_reply":"2023-12-17T19:20:40.961474Z"},"trusted":true},"execution_count":284,"outputs":[]},{"cell_type":"code","source":"def forward_prop(X, parameters):\n    \"\"\"\n    Implement forward propagation\n\n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters()\n\n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n    caches = []\n    A = X\n    L = len(parameters) // 2  # Dividing by 2 because parameters include W and b for each layer\n    \n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, \n                                             parameters['W' + str(l)], \n                                             parameters['b' + str(l)], \n                                             activation='relu')\n        caches.append(cache)\n    \n    # Implement LINEAR -> SOFTMAX for the last layer.\n    AL, cache = linear_activation_forward(A, \n                                          parameters['W' + str(L)], \n                                          parameters['b' + str(L)], \n                                          activation='softmax')\n    caches.append(cache)\n          \n    return AL, caches\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.963748Z","iopub.execute_input":"2023-12-17T19:20:40.964452Z","iopub.status.idle":"2023-12-17T19:20:40.973206Z","shell.execute_reply.started":"2023-12-17T19:20:40.964423Z","shell.execute_reply":"2023-12-17T19:20:40.972294Z"},"trusted":true},"execution_count":285,"outputs":[]},{"cell_type":"code","source":"def compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function.\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector, shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    m = Y.shape[1]\n    epsilon = 1e-8\n\n    # Compute the cross-entropy cost with epsilon to avoid log(0)\n    cost = -np.sum(Y * np.log(AL + epsilon)) / m\n    cost = np.squeeze(cost)  # To make sure the cost's shape is what we expect.\n\n    return cost","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.974566Z","iopub.execute_input":"2023-12-17T19:20:40.975080Z","iopub.status.idle":"2023-12-17T19:20:40.986533Z","shell.execute_reply.started":"2023-12-17T19:20:40.975055Z","shell.execute_reply":"2023-12-17T19:20:40.985920Z"},"trusted":true},"execution_count":286,"outputs":[]},{"cell_type":"code","source":"def relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    \n    # When z <= 0, you should set dz to 0 as well. \n    dZ[Z <= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:40.996898Z","iopub.execute_input":"2023-12-17T19:20:40.997274Z","iopub.status.idle":"2023-12-17T19:20:41.000956Z","shell.execute_reply.started":"2023-12-17T19:20:40.997253Z","shell.execute_reply":"2023-12-17T19:20:41.000401Z"},"trusted":true},"execution_count":287,"outputs":[]},{"cell_type":"code","source":"def softmax_backward(AL, Y, cache):\n    \"\"\"\n    Implements the backward propagation for a single softmax unit.\n\n    Arguments:\n    AL -- post-activation output from the softmax layer, of shape (number of classes, number of examples)\n    Y -- true \"label\" vector, same shape as AL\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    Z = cache\n    dZ = AL - Y\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.002575Z","iopub.execute_input":"2023-12-17T19:20:41.003573Z","iopub.status.idle":"2023-12-17T19:20:41.014802Z","shell.execute_reply.started":"2023-12-17T19:20:41.003546Z","shell.execute_reply":"2023-12-17T19:20:41.014205Z"},"trusted":true},"execution_count":288,"outputs":[]},{"cell_type":"code","source":"def linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = 1./m * np.dot(dZ, A_prev.T)\n    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.015855Z","iopub.execute_input":"2023-12-17T19:20:41.016781Z","iopub.status.idle":"2023-12-17T19:20:41.025895Z","shell.execute_reply.started":"2023-12-17T19:20:41.016750Z","shell.execute_reply":"2023-12-17T19:20:41.024964Z"},"trusted":true},"execution_count":289,"outputs":[]},{"cell_type":"code","source":"def linear_activation_backward(dA, Y, cache, activation):\n    \"\"\"\n    Implements the backward propagation for the LINEAR->ACTIVATION layer.\n\n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"softmax\" or \"relu\"\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    match activation:\n        case \"softmax\":\n            dZ = softmax_backward(dA, Y, activation_cache)\n            dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        case \"relu\":\n            dZ = relu_backward(dA, activation_cache)\n            dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        case _:\n            assert (False)\n        \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.027050Z","iopub.execute_input":"2023-12-17T19:20:41.027599Z","iopub.status.idle":"2023-12-17T19:20:41.035757Z","shell.execute_reply.started":"2023-12-17T19:20:41.027576Z","shell.execute_reply":"2023-12-17T19:20:41.035225Z"},"trusted":true},"execution_count":290,"outputs":[]},{"cell_type":"code","source":"def back_prop(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation\n\n    Arguments:\n    AL -- probability vector, output of the forward propagation\n    Y -- true \"label\" vector\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"softmax\" (it's caches[L-1])\n\n    Returns:\n    grads -- A dictionary with the gradients\n        grads[\"dA\" + str(l)] = ... \n        grads[\"dW\" + str(l)] = ...\n        grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    # Initializing the backpropagation\n    epsilon = 1e-8 # to avoid division by zero\n    dAL = -(np.divide(Y, AL + epsilon) - np.divide(1 - Y, 1 - AL + epsilon))\n    \n    # Lth layer gradients.\n    current_cache = caches[-1]\n    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, Y, current_cache, activation='softmax')\n    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n    grads[\"dW\" + str(L)] = dW_temp\n    grads[\"db\" + str(L)] = db_temp\n    \n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], Y, current_cache, activation='relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n\n    return grads","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.037358Z","iopub.execute_input":"2023-12-17T19:20:41.037579Z","iopub.status.idle":"2023-12-17T19:20:41.049572Z","shell.execute_reply.started":"2023-12-17T19:20:41.037559Z","shell.execute_reply":"2023-12-17T19:20:41.048842Z"},"trusted":true},"execution_count":291,"outputs":[]},{"cell_type":"code","source":"def update_parameters(params, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent.\n\n    Arguments:\n    params -- python dictionary containing the parameters \n    grads -- python dictionary containing the gradients, output of backwards propagation\n\n    Returns:\n    parameters -- python dictionary containing the updated parameters \n        parameters[\"W\" + str(l)] = ... \n        parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    parameters = copy.deepcopy(params)\n    L = len(parameters) // 2\n\n    for l in range(L):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.050405Z","iopub.execute_input":"2023-12-17T19:20:41.051143Z","iopub.status.idle":"2023-12-17T19:20:41.063433Z","shell.execute_reply.started":"2023-12-17T19:20:41.051120Z","shell.execute_reply":"2023-12-17T19:20:41.062688Z"},"trusted":true},"execution_count":292,"outputs":[]},{"cell_type":"code","source":"def model(X, Y, layers_dims, initial_learning_rate=0.01, decay=10, decay_interval=50, num_iterations=500, print_cost=True):\n    \"\"\"\n    Implements a L-layer neural network\n\n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    decay -- The learning rate decay\n    decay_interval -- Specifies on this iteration the decay is applied to the learning rate\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    costs = []\n    learning_rate = initial_learning_rate\n    \n    # Parameters initialization.\n    parameters = initialize_parameters(layers_dims)\n    \n    # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Learning rate decay by factor of decay for every decay_iteration iterations\n        if i > 0 and i % decay_interval == 0:\n            learning_rate /= decay\n        \n        AL, caches = forward_prop(X, parameters)\n        cost = compute_cost(AL, Y)\n        grads = back_prop(AL, Y, caches)\n        parameters = update_parameters(parameters, grads, learning_rate)\n                \n        # Print the cost every decay_interval iterations\n        if print_cost and i % decay_interval == 0 or i == num_iterations - 1:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n            print(\"learning_rate after iteration {}: {}\".format(i, learning_rate))\n            \n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n    \n    return parameters, costs","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.065588Z","iopub.execute_input":"2023-12-17T19:20:41.066223Z","iopub.status.idle":"2023-12-17T19:20:41.076810Z","shell.execute_reply.started":"2023-12-17T19:20:41.066190Z","shell.execute_reply":"2023-12-17T19:20:41.076063Z"},"trusted":true},"execution_count":293,"outputs":[]},{"cell_type":"markdown","source":"# Using the custom deep neural network\n\n## Read data from CSV","metadata":{}},{"cell_type":"code","source":"filepath = '/kaggle/input/digit-recognizer/train.csv'\nraw_data = pd.read_csv(filepath)\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:41.077967Z","iopub.execute_input":"2023-12-17T19:20:41.078234Z","iopub.status.idle":"2023-12-17T19:20:42.616252Z","shell.execute_reply.started":"2023-12-17T19:20:41.078211Z","shell.execute_reply":"2023-12-17T19:20:42.615425Z"},"trusted":true},"execution_count":294,"outputs":[{"execution_count":294,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare the data","metadata":{}},{"cell_type":"code","source":"X, y = data_prep(raw_data)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:20:42.618021Z","iopub.execute_input":"2023-12-17T19:20:42.618253Z","iopub.status.idle":"2023-12-17T19:20:42.714524Z","shell.execute_reply.started":"2023-12-17T19:20:42.618232Z","shell.execute_reply":"2023-12-17T19:20:42.713690Z"},"trusted":true},"execution_count":295,"outputs":[{"name":"stdout","text":"data.shape = (m = 42000, n = 785)\nX.shape = (784, 42000)\ny_one_hot.shape = (10, 42000)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Define the model","metadata":{}},{"cell_type":"code","source":"# Hyperparameters defining the model\nlayers_dims = [len(X), int(len(X) / 2), 10]\nlearning_rate_init = 0.01\nlearning_rate_decay = 5\nlearning_rate_decay_interval = 25\nnum_iterations = 125\n\n# Generate the model\nparameters, costs = model(X=X,\n                          Y=y,\n                          layers_dims=layers_dims,\n                          initial_learning_rate=learning_rate_init,\n                          decay=learning_rate_decay,\n                          decay_interval=learning_rate_decay_interval,\n                          num_iterations=num_iterations,\n                          print_cost=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:26:16.549370Z","iopub.execute_input":"2023-12-17T19:26:16.549667Z","iopub.status.idle":"2023-12-17T19:28:45.417250Z","shell.execute_reply.started":"2023-12-17T19:26:16.549645Z","shell.execute_reply":"2023-12-17T19:28:45.416354Z"},"trusted":true},"execution_count":299,"outputs":[{"name":"stdout","text":"Cost after iteration 0: 2.4342365755579816\nlearning_rate after iteration 0: 0.01\nCost after iteration 25: 0.8588547377958773\nlearning_rate after iteration 25: 0.002\nCost after iteration 50: 0.7384982755077013\nlearning_rate after iteration 50: 0.0004\nCost after iteration 75: 0.7195299970138487\nlearning_rate after iteration 75: 8e-05\nCost after iteration 100: 0.7159287218162212\nlearning_rate after iteration 100: 1.6000000000000003e-05\nCost after iteration 124: 0.7152440080475847\nlearning_rate after iteration 124: 1.6000000000000003e-05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predict on training set","metadata":{}},{"cell_type":"code","source":"def predict_train_data(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a L-layer neural network.\n\n    Arguments:\n    X -- data set of examples you would like to label\n    y -- true \"label\" vector (one-hot encoded)\n    parameters -- parameters of the trained model\n\n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    m = X.shape[1]\n    n = len(parameters) // 2\n\n    # Forward propagation\n    AL, caches = forward_prop(X, parameters)\n\n    # Convert predictions and true labels to categorical form\n    predictions = np.argmax(AL, axis=0)\n    true_labels = np.argmax(y, axis=0)\n\n    # Calculate accuracy\n    accuracy = np.sum(predictions == true_labels) / m\n\n    # Print results\n    print(\"Accuracy: \" + str(accuracy * 100) + '%')\n\n    return predictions\n\n# Make the predicitons on training data set\npred_train = predict_train_data(X, y, parameters)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:29:55.750385Z","iopub.execute_input":"2023-12-17T19:29:55.750701Z","iopub.status.idle":"2023-12-17T19:29:56.145573Z","shell.execute_reply.started":"2023-12-17T19:29:55.750678Z","shell.execute_reply":"2023-12-17T19:29:56.144920Z"},"trusted":true},"execution_count":300,"outputs":[{"name":"stdout","text":"Accuracy: 81.16190476190475%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predict using test set and plot some random predictions","metadata":{}},{"cell_type":"code","source":"def prepare_test_data(raw):\n    \"\"\"\n    Prepares test data (without labels)\n\n    Arguments:\n    raw -- raw test data read from a csv file\n\n    Returns:\n    X -- normalized test data\n    \"\"\"\n    X = np.array(raw) / 255\n    X = X.T  # Transpose to match the expected shape (number_of_features, number_of_examples)\n    \n    print(\"X_test.shape =\", X.shape)\n    \n    return X\n\ndef predict_test_data(X, parameters):\n    \"\"\"\n    This function is used to predict the results of a L-layer neural network.\n\n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n\n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    m = X.shape[1]\n    n = len(parameters) // 2\n\n    # Forward propagation\n    AL, caches = forward_prop(X, parameters)\n\n    # Convert predictions to categorical form\n    predictions = np.argmax(AL, axis=0)\n\n    return predictions\n\n# Read and prepare test data\nfilepath = '/kaggle/input/digit-recognizer/test.csv'\nraw_test_data = pd.read_csv(filepath)\nX_test = prepare_test_data(raw_test_data)\n\n# Make the predicitons on test data set\npred_test = predict_test_data(X_test, parameters)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:30:00.899511Z","iopub.execute_input":"2023-12-17T19:30:00.899825Z","iopub.status.idle":"2023-12-17T19:30:02.423952Z","shell.execute_reply.started":"2023-12-17T19:30:00.899803Z","shell.execute_reply":"2023-12-17T19:30:02.423082Z"},"trusted":true},"execution_count":301,"outputs":[{"name":"stdout","text":"X_test.shape = (784, 28000)\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_images_with_predictions(X, predictions, num_images=10):\n    \"\"\"\n    Plots a random selection of images with their predicted labels.\n\n    Arguments:\n    X -- dataset of shape (number_of_features, number_of_examples)\n    predictions -- array of predicted labels for the dataset\n    num_images -- number of random images to display\n    \"\"\"\n    fig, axes = plt.subplots(1, num_images, figsize=(20, 4))\n    \n    for i, ax in enumerate(axes):\n        # Randomly select an image\n        idx = np.random.randint(X.shape[1])\n        img = X[:, idx].reshape(28, 28)  # Reshape the image to 28x28\n\n        ax.imshow(img, cmap='gray')\n        ax.set_title(f'Predicted: {predictions[idx]}')\n        ax.axis('off')\n\n    plt.show()\n\n# Plot images with predictions\nplot_images_with_predictions(X_test, pred_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:37:56.871441Z","iopub.execute_input":"2023-12-17T19:37:56.872604Z","iopub.status.idle":"2023-12-17T19:37:57.359508Z","shell.execute_reply.started":"2023-12-17T19:37:56.872567Z","shell.execute_reply":"2023-12-17T19:37:57.358647Z"},"trusted":true},"execution_count":306,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 2000x400 with 10 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create file of predictions for submission","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame with ImageId and Label\nresult_df = pd.DataFrame({\n    'ImageId': range(1, len(pred_test) + 1),\n    'Label': pred_test\n})\n\n# Save to CSV\nfilename = \"submission.csv\"\nresult_df.to_csv(filename, index=False)\n\nprint(f'Saved predictions to {filename}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:37:23.663267Z","iopub.execute_input":"2023-12-17T19:37:23.663617Z","iopub.status.idle":"2023-12-17T19:37:23.697001Z","shell.execute_reply.started":"2023-12-17T19:37:23.663591Z","shell.execute_reply":"2023-12-17T19:37:23.696169Z"},"trusted":true},"execution_count":305,"outputs":[{"name":"stdout","text":"Saved predictions to submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}